{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.7\n",
    "\n",
    "# you can install the ES library using pip\n",
    "import elasticsearch \n",
    "\n",
    "# elasticsearch also have a higher level api (elasticsearch_dsl)\n",
    "# its documentation is lacking and doesn't support all ES features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The schema I used to index wikipedia text is somewhat like this json below,\n",
    "# each doc has one title, many sections(each with a header and a body)\n",
    "# doc:{\n",
    "#   title: Text\n",
    "#   sections:[ {header: Text, body: Text}, ... ]\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init connection to nlp12\n",
    "client=elasticsearch.Elasticsearch([{'host':'nlp12','port':9200}],timeout=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use spacy for sentence segmentation\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#and some regexs\n",
    "import re\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hits: 10000\n",
      "(10k is the default max.)\n",
      "\n",
      "The following pages were scored according to \"french\" relevence in their section (ranking algorithm called bs25):\n",
      "\tAmerican French: Introduction   -   ...'''American French''' (French: le '''''fra...\n",
      "\tSection d'Or: Notable members   -   ...garian, naturalized French *Alexandra Exter, 1882–...\n",
      "\tCo-ordinated Organisation: Linguistic rules of the coordinated organisations   -   ...llon, member of the French National Assembly....\n",
      "\tFrench music: Introduction   -   ...'''''French music''''' may refer to...\n",
      "\tÉdouard: Introduction   -   ...douard''' is both a French given name and a surnam...\n"
     ]
    }
   ],
   "source": [
    "# get documents containing \"french\" accroding to relevence\n",
    "response = client.search(\n",
    "    index=\"wiki\", # This is somewhat like the SQL \"table\" \n",
    "    params={'from':0,'size':5}, # get 5 highest scroed\n",
    "    body={'query':\n",
    "              {\"nested\": { # search within document sections\n",
    "                  'path':'sections',\n",
    "                  'inner_hits':{},# return which section matched\n",
    "                  \"query\": {\"match\": {\"sections.body\": 'french'}}\n",
    "                \n",
    "              }}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Total hits: {response['hits']['total']['value']}\")\n",
    "print('(10k is the default max.)')\n",
    "print()\n",
    "print('The following pages were scored according to \"french\" relevence in their section (ranking algorithm called bs25):')\n",
    "for hit in response['hits']['hits']:\n",
    "    page_title=hit['_source']['title']\n",
    "    matched_section=hit['inner_hits']['sections']['hits']['hits'][0]['_source']\n",
    "    position=matched_section['body'].lower().index('french')\n",
    "    text=matched_section['body'][max(0,position-20):position+30].replace('\\n',' ').strip()\n",
    "    print(f'\\t{page_title}: {matched_section[\"header\"]}   -   ...{text}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hits: 870\n",
      "\n",
      "hits:\n",
      "\tSir Kensington's: Fries of New York - ...bition featured 100 French fries from New York Cit...\n",
      "\tSuperdawg: The food - ...The hot dog and French fries are served togeth...\n",
      "\tBeverly Hills Caviar Automated Boutique: See also - ...* French fries vending machine ...\n",
      "\tBlooming onion: See also - ... *French fries * List of hors d'...\n",
      "\tList of cultural icons of the Netherlands: Food and Drink - ... *Heineken *Gouda  *French fries *Stroopwafel ...\n"
     ]
    }
   ],
   "source": [
    "#now a pharse: \"french fries\"\n",
    "response = client.search(\n",
    "    index=\"wiki\", \n",
    "    params={'from':0,'size':5}, # get 5 highest scroed\n",
    "    body={'query':\n",
    "              {\"nested\": { # search within document sections\n",
    "                  'path':'sections',\n",
    "                  'inner_hits':{},# return which section matched\n",
    "                  \"query\": {\"match_phrase\": {\"sections.body\": 'french fries'}}\n",
    "              }}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Total hits: {response['hits']['total']['value']}\")\n",
    "print()\n",
    "print('hits:')\n",
    "for hit in response['hits']['hits']:\n",
    "    page_title=hit['_source']['title']\n",
    "    matched_section=hit['inner_hits']['sections']['hits']['hits'][0]['_source']\n",
    "    position=matched_section['body'].lower().index('french fries')\n",
    "    text=matched_section['body'][max(0,position-20):position+30].replace('\\n',' ')\n",
    "    print(f'\\t{page_title}: {matched_section[\"header\"]} - ...{text}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hits: 870\n",
      "\n",
      "The refined peanut oil has a smoke point of 450 °F/232 °C is commonly used for frying volume batches foods like french fries.\n"
     ]
    }
   ],
   "source": [
    "#same pharse \"french fries\", but let's get a random sentence containing it\n",
    "some_random_seed=5\n",
    "\n",
    "response = client.search(\n",
    "    index=\"wiki\", \n",
    "    params={'from':0,'size':1}, # get 1\n",
    "    body={'query':\n",
    "              {\"nested\": { # search within document sections\n",
    "                  'path':'sections',\n",
    "                  'inner_hits':{},# return which section matched\n",
    "                  \"query\": {\"bool\": {# these \"bool\" queres allows combining mutiple queries in various ways\n",
    "                      \"filter\": [\n",
    "                        {\"match_phrase\": {\"sections.body\": 'french fries'}}\n",
    "                      ],\n",
    "                      \"must\": {\n",
    "                        \"function_score\": {\n",
    "                            \"functions\": [\n",
    "                                {\n",
    "                                    \"random_score\": {\"seed\": some_random_seed}\n",
    "                                }\n",
    "                            ],\n",
    "                            \"boost_mode\": \"replace\"\n",
    "                          }\n",
    "                        }\n",
    "                  }\n",
    "                } \n",
    "              }}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Total hits: {response['hits']['total']['value']}\")\n",
    "print()\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    page_title=hit['_source']['title']\n",
    "    matched_section=hit['inner_hits']['sections']['hits']['hits'][0]['_source']\n",
    "    section_body=matched_section['body']\n",
    "    for paragraph in section_body.split('\\n'):\n",
    "        parsed=nlp(paragraph)\n",
    "        for sentence in parsed.sents:\n",
    "            if 'french fries' in sentence.lower_:\n",
    "                print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hits 405\n",
      "\n",
      "found in: \n",
      "\tPlata v. Schwarzenegger: Reactions\n",
      "\t2004 in LGBT rights: Events\n",
      "\tList of Republicans who opposed the 2016 Donald Trump presidential campaign: Government officials\n",
      "\tFirst term of Arnold Schwarzenegger as Governor of California: First term\n",
      "\tKevin Cooper (prisoner): Assessment by courts, governors, and independent groups\n",
      "\t...\n",
      "\n",
      "common words between: arnold, governor, of, the, schwarzenegger\n"
     ]
    }
   ],
   "source": [
    "# find words between occurences of  \"schwarzenegger\" and \"california\"\n",
    "response = client.search(\n",
    "    index=\"wiki\", \n",
    "    params={'from': 0, 'size': 200}, # first 100 hits \n",
    "    body={'query':\n",
    "        {\"nested\": {\n",
    "            'path': 'sections',\n",
    "            'inner_hits': {},\n",
    "            \"query\": {\n",
    "                \"intervals\": { # find text with both tokens: schwarzenegger and california \n",
    "                    \"sections.body\": {\n",
    "                        \"all_of\": {\n",
    "                            \"max_gaps\" : 2, # max. 2 words between tokens\n",
    "                            \"intervals\" : [\n",
    "                                {\n",
    "                                  \"match\" : {\n",
    "                                    \"query\" : \"schwarzenegger\",\n",
    "                                  }},\n",
    "                                 {\n",
    "                                  \"match\" : {\n",
    "                                    \"query\" : \"california\",\n",
    "                                  }}\n",
    "                            ]\n",
    "                            },\n",
    "\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }}\n",
    ")\n",
    "\n",
    "print(f\"Total hits {response['hits']['total']['value']}\\n\")\n",
    "foundin=[]\n",
    "words_inbetween=Counter()\n",
    "for hit in response['hits']['hits']:\n",
    "    page_title=hit['_source']['title']\n",
    "    matched_section=hit['inner_hits']['sections']['hits']['hits'][0]['_source']\n",
    "    matches=re.findall('(california|schwarzenegger)([a-z ]{0,100})(california|schwarzenegger)',matched_section['body'].lower() )\n",
    "    if matches:\n",
    "        mathced_mid = matches[0][1].strip()\n",
    "        if mathced_mid:\n",
    "            words_inbetween.update(mathced_mid.split())\n",
    "            foundin.append(f'{page_title}: {matched_section[\"header\"]}')\n",
    "print('found in: \\n\\t'+'\\n\\t'.join(foundin[:5])+'\\n\\t...')\n",
    "print()\n",
    "print('common words between: '+', '.join( w for w,_ in words_inbetween.most_common(5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated over 21260869 documents\n",
      "We have full term freq. in the matched docs:\n",
      "the, of, and, in, a, to, for, by, as, on ... state, several, high, until, 3, service, before, system, second, business ...\n"
     ]
    }
   ],
   "source": [
    "# find term freqency in documents containg 'acquisition' - i.e. probably related to economics\n",
    "response = client.search(\n",
    "    index=\"wiki\", \n",
    "    params={'from':0,'size':1},\n",
    "    body={'query':\n",
    "              {\"nested\": {\n",
    "                  'path':'sections',\n",
    "                  \"query\": {\"match\": {\"sections.body\": 'acquisition'}}\n",
    "                \n",
    "              }},\n",
    "          'aggs': {'wf': {'nested': {'path': 'sections'},\n",
    "           'aggs': {'wf2': {'terms': {'field': 'sections.body', 'size': 1000}}}}}\n",
    "          \n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Aggregated over {response['aggregations']['wf']['wf2']['sum_other_doc_count']} documents\")\n",
    "print('We have full term freq. in the matched docs:')\n",
    "print(', '.join([x['key'] for x in response['aggregations']['wf']['wf2']['buckets'][:10] ])+' ... '+ \\\n",
    "', '.join([x['key'] for x in response['aggregations']['wf']['wf2']['buckets'][100:110] ])+' ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for refernce this is the full schema\n",
    "# Kibana is also running on nlp12:5601\n",
    "    \n",
    "#     {\n",
    "#   \"mapping\": {\n",
    "#     \"properties\": {\n",
    "#       \"hyperlinks\": {\n",
    "#         \"type\": \"text\",\n",
    "#         \"fields\": {\n",
    "#           \"keyword\": {\n",
    "#             \"type\": \"keyword\",\n",
    "#             \"ignore_above\": 256\n",
    "#           }\n",
    "#         }\n",
    "#       },\n",
    "#       \"sections\": {\n",
    "#         \"type\": \"nested\",\n",
    "#         \"properties\": {\n",
    "#           \"body\": {\n",
    "#             \"type\": \"text\",\n",
    "#             \"fields\": {\n",
    "#               \"stemmed\": {\n",
    "#                 \"type\": \"text\",\n",
    "#                 \"analyzer\": \"snowball\"\n",
    "#               }\n",
    "#             },\n",
    "#             \"fielddata\": true\n",
    "#           },\n",
    "#           \"header\": {\n",
    "#             \"type\": \"text\"\n",
    "#           },\n",
    "#           \"hyperlinks\": {\n",
    "#             \"type\": \"text\"\n",
    "#           }\n",
    "#         }\n",
    "#       },\n",
    "#       \"title\": {\n",
    "#         \"type\": \"text\"\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1",
   "language": "python",
   "name": "torch1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
